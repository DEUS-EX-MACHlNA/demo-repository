# 게임 서버 배포 가이드 (최신화)

이 문서는 게임 서버를 프로덕션 환경에 배포하기 위한 현재의 아키텍처와 배포 프로세스를 설명합니다. 기존의 단일 머신(All-in-One) 형태에서 발전하여 **웹/DB 인프라와 무거운 LLM 연산 노드를 분리한 구조**를 채택하고 있습니다.

## 1. 아키텍처: 웹/DB 서버와 GPU(LLM) 서버의 분리

FastAPI 서버, PostgreSQL 데이터베이스, Redis 캐시는 메인 호스팅 서버(예: AWS EC2 등)에서 단일 Docker Compose 환경으로 구동되며, 고사양이 필요한 AI 모델(vLLM)은 별도의 GPU 장비(로컬 또는 클라우드)에서 구동되어 ngrok 등의 터널링을 통해 API로 통신합니다.

### 시스템 구성 요소 (Docker Compose 워크로드)

이 컴포넌트들은 `docker-compose.prod.yml`을 통해 **메인 호스팅 서버** 내에 배포됩니다.

| 서비스 | 컨테이너 이름 | 역할 |
| :--- | :--- | :--- |
| **Nginx** | `nginx` | **리버스 프록시**: 포트 80 트래픽 처리 및 트래픽 분배 |
| **FastAPI** | `app` | **게임 메인 서버**: 게임 로직 수행, DB/Redis 통신 및 외부 LLM API 호출 |
| **PostgreSQL** | `db` | **데이터베이스**: 사용자 정보, 로그, 통계 기록 영구 저장 |
| **Redis** | `redis` | **인메모리 캐시**: 턴 진행 및 게임 상태 고속 임시 저장 (`O(1)` 속도) |
| **초기화 워커** | `init_db` | **초기 기동 (1회성)**: DB 테이블 마이그레이션(`alembic`) 및 시나리오 메타데이터 로더 |

---

### 외부 의존성 (External Workloads)
| 서비스 | 역할 |
| :--- | :--- |
| **vLLM (기본)** | 시나리오 생성 및 일반 NPC 응답 텍스트를 생성하는 API 서빙. |
| **vLLM (LoRA)** | 특정 캐릭터나 상황에 맞게 튜닝된 모델(LoRA)용 API 서빙. |
*(이 두 모델 엔드포인트는 `.env.prod`의 `VLLM_BASE_URL`, `LORA_VLLM_BASE_URL` 파라미터를 통해 주입됩니다.)*

---

### 아키텍처 다이어그램

```text
       [ 사용자 (User/Player) ]
                  |
                  | HTTP:80 액션/턴 진행 요청
                  v
+---------------------------------------------------------+
| 메인 서버 (AWS EC2 등) / Docker Host                    |
|                                                         |
|  [ 내부 네트워크: maratang_net ]                        |
|                                                         |
|  +---------------+       +---------------------------+  |
|  |  Nginx (:80)  | ----> |   FastAPI App (:8000)     |  |
|  +---------------+       +---+-------------------+---+  |
|                              |                   |      |
|   +-----------------------+  | 고속 캐싱 (O(1))  |      |
|   | Init DB Worker (1회)  |  v                   |      |
|   +-----------+-----------+ +-------+            |      |
|               |             | Redis |            |      |
|          테이블 생성         +-------+            | 영구 |
|         마이그레이션                              | 저장 |
|               |                                  |      |
|               v                                  v      |
|          +---------------------------------------+      |
|          |         PostgreSQL 16 (:5432)         |      |
|          +---------------------------------------+      |
+-----------------------+----------+----------------------+
                        |          |
                 REST API (텍스트 생성 요청)
                        |          |
                        v          v
+-----------------------+----------+----------------------+
| 외부 GPU 서버군 (RunPod, LocalPC, ngrok 등)             |
|                                                         |
|    +--------------------+     +--------------------+    |
|    |    기본 vLLM       |     |     LoRA vLLM      |    |
|    +--------------------+     +--------------------+    |
+---------------------------------------------------------+
```

### 데이터 흐름
1. **사용자**가 게임 클라이언트에서 액션을 보냅니다.
2. **Nginx**가 요청을 받아 내부망의 `app(FastAPI)`으로 넘깁니다.
3. **App**은 **Redis**에서 해당 사용자의 현재 게임 상태를 밀리초 단위로 가져옵니다.
4. AI 텍스트 생성이 필요한 경우, App은 외부 `VLLM_BASE_URL`로 HTTP 텍스트 생성 요청을 전송합니다.
5. 응답을 받으면 **Redis**에 새 상태를 업데이트하고, 변동 내역은 백그라운드 스케줄러를 통해 **DB**로 비동기 동기화합니다.
6. 최종 응답값( JSON 등)이 사용자에게 반환됩니다.

---

## 2. 배포 단계 (수동 배포 방식)

현재 자동화된 CI/CD 파이프라인(GitHub Actions)은 일시 중지된 상태이며, 서버에 직접 접속해 업데이트 스크립트를 수행하는 방식(Manual Pull & Build)을 사용합니다.

### 1단계: 서버 환경변수 세팅 (`.env.prod`)
가장 먼저 제품 환경(Production)에서 사용할 **기밀 및 접속 정보**를 세팅해야 합니다.
프로젝트 루트 디렉토리에 `.env.prod` 파일을 생성하고 다음을 기입합니다.
> **주의:** `.env.prod`는 Git에 안전상의 이유로 커밋되지 않습니다. 코드가 업데이트되어도 서버 내에 유지되어야 합니다.

```ini
# DB 접속 정보
POSTGRES_USER=maratang_admin
POSTGRES_PASSWORD=your_secure_password
POSTGRES_DB=maratang_db
TZ=Asia/Seoul

# 외부 LLM(GPU) 통신용 URL
VLLM_BASE_URL="https://your-base-vllm-url.ngrok-free.app/"
LORA_VLLM_BASE_URL="https://your-lora-vllm-url.ngrok-free.app/"
```

### 2단계: 자동화 스크립트를 통한 배포 (`start_prod.sh`)
메인 서버(리눅스/Ubuntu) 터미널에 접속하여 코드를 최신화한 후 `start_prod.sh`를 실행하기만 하면 전체 시스템이 가동됩니다.

```bash
# 1. 터미널 접속 (예: ssh ubuntu@your_server_ip)
# 2. 프로젝트 폴더로 이동
cd ~/demo-repository

# 3. 최신 코드 가져오기
git pull origin main

# 4. 배포 스크립트 실행 (도커 빌드 및 데몬 재시작)
sudo sh ./start_prod.sh
```

**`start_prod.sh`의 역할:**
1. Host 머신 설정용 conda 환경(`deus`) 강제 로드
2. `docker-compose.prod.yml` 파일과 `.env.prod` 변수 파일을 읽어 시스템 준비 (기존 컨테이너 Down)
3. DB 초기화 컨테이너 수행 후 메인 `app` 컨테이너 백그라운드 구동 (상태 마이그레이션 포함)

---

## 3. 요약 및 운영 관리

- **LLM 서버 다운 시**: AI 서버 응답이 실패하면 메인 로직(`app`)에서 Timeout 에러가 떨어지므로 관리자가 즉시 ngrok URL 만료 여부나 vLLM 장비 상태를 체크하여 `.env.prod` 파일을 수정 후 조치해야 합니다. (수정 후 `sudo sh start_prod.sh` 재가동)
- **DB 백업 전략**: `postgres_data` 볼륨이 영구 스토리지로 마운트되어 정보 증발을 막지만, 인스턴스 손상에 대비한 별도 논리 백업(`pg_dump`)을 권장합니다.
- **모니터링**: Nginx, App, DB 컨테이너 로그는 모두 도커가 관장합니다. 
  ```bash
  # 서비스 전체 로그 실시간 확인
  docker-compose --env-file .env.prod -f docker-compose.prod.yml logs -f
  ```
