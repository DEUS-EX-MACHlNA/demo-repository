{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tools.py ë””ë²„ê¹… í…ŒìŠ¤íŠ¸\n",
    "\n",
    "tool_turn_resolution í•¨ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€\n",
    "root = Path.cwd()\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))\n",
    "\n",
    "print(f\"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s %(name)s %(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2. GPU / ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ í™•ì¸",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\n\nprint(\"=\" * 60)\nprint(\"GPU / ë¡œì»¬ ëª¨ë¸ í™•ì¸\")\nprint(\"=\" * 60)\n\n# 1. CUDA ê°€ìš©ì„±\nprint(f\"\\n[1] torch.cuda.is_available(): {torch.cuda.is_available()}\")\nprint(f\"    torch.version.cuda: {torch.version.cuda}\")\nprint(f\"    torch.__version__: {torch.__version__}\")\n\n# 2. GPU ì •ë³´\nif torch.cuda.is_available():\n    print(f\"\\n[2] GPU ì •ë³´:\")\n    print(f\"    device_count: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"    GPU {i}: {torch.cuda.get_device_name(i)}\")\n        props = torch.cuda.get_device_properties(i)\n        print(f\"        ì´ ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.2f} GB\")\n    \n    # í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n    print(f\"\\n[3] GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (í˜„ì¬):\")\n    print(f\"    allocated: {torch.cuda.memory_allocated() / 1024**3:.3f} GB\")\n    print(f\"    reserved:  {torch.cuda.memory_reserved() / 1024**3:.3f} GB\")\nelse:\n    print(\"\\n[!] CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ë™ì‘ ì¤‘.\")\n    print(\"    GPUê°€ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì´ìœ ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n\n# 3. nvidia-smi ì •ë³´ (subprocess)\nprint(f\"\\n[4] nvidia-smi í™•ì¸:\")\nimport subprocess\ntry:\n    result = subprocess.run(\n        [\"nvidia-smi\", \"--query-gpu=name,memory.used,memory.total,utilization.gpu\", \"--format=csv,noheader,nounits\"],\n        capture_output=True, text=True, timeout=5\n    )\n    if result.returncode == 0:\n        for line in result.stdout.strip().split(\"\\n\"):\n            parts = [p.strip() for p in line.split(\",\")]\n            if len(parts) >= 4:\n                name, mem_used, mem_total, util = parts\n                print(f\"    {name}: {mem_used}/{mem_total} MB, GPU ì‚¬ìš©ë¥ : {util}%\")\n    else:\n        print(f\"    nvidia-smi ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}\")\nexcept FileNotFoundError:\n    print(\"    nvidia-smië¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\nexcept Exception as e:\n    print(f\"    nvidia-smi ì‹¤í–‰ ì˜¤ë¥˜: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.loader import ScenarioLoader\n",
    "\n",
    "base_path = root / \"scenarios\"\n",
    "loader = ScenarioLoader(base_path)\n",
    "scenario_ids = loader.list_scenarios()\n",
    "\n",
    "print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤: {scenario_ids}\")\n",
    "\n",
    "if not scenario_ids:\n",
    "    raise Exception(\"ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "scenario_id = scenario_ids[0]\n",
    "assets = loader.load(scenario_id)\n",
    "print(f\"\\në¡œë“œëœ ì‹œë‚˜ë¦¬ì˜¤: {scenario_id}\")\n",
    "print(f\"  - ì œëª©: {assets.scenario.get('title')}\")\n",
    "print(f\"  - NPC: {assets.get_all_npc_ids()}\")\n",
    "print(f\"  - ì•„ì´í…œ: {assets.get_all_item_ids()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì›”ë“œ ìƒíƒœ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.models import WorldState, NPCState\n",
    "\n",
    "world = WorldState(\n",
    "    turn=1,\n",
    "    npcs={\n",
    "        \"family\": NPCState(npc_id=\"family\", trust=0, fear=0, suspicion=0),\n",
    "        \"partner\": NPCState(npc_id=\"partner\", trust=0, fear=0, suspicion=1),\n",
    "        \"witness\": NPCState(npc_id=\"witness\", trust=0, fear=2, suspicion=0),\n",
    "    },\n",
    "    inventory=[\"casefile_brief\", \"pattern_analyzer\"],\n",
    "    vars={\"clue_count\": 0, \"fabrication_score\": 0},\n",
    ")\n",
    "\n",
    "print(f\"WorldState:\")\n",
    "print(f\"  - turn: {world.turn}\")\n",
    "print(f\"  - npcs: {list(world.npcs.keys())}\")\n",
    "print(f\"  - inventory: {world.inventory}\")\n",
    "print(f\"  - vars: {world.vars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from app.llm import LLM_Engine\n\nmodel_name = os.environ.get(\"LLM_MODEL\", \"Qwen/Qwen3-8B\")\nprint(f\"LLM ë¡œë”© ì¤‘: {model_name}\")\nprint(\"(í™˜ê²½ë³€ìˆ˜ LLM_MODELë¡œ ë³€ê²½ ê°€ëŠ¥)\")\n\n# GPU ë©”ëª¨ë¦¬ ë¡œë“œ ì „ ì¸¡ì •\nif torch.cuda.is_available():\n    torch.cuda.synchronize()\n    mem_before = torch.cuda.memory_allocated()\n    print(f\"\\n[ëª¨ë¸ ë¡œë“œ ì „] GPU ë©”ëª¨ë¦¬: {mem_before / 1024**3:.3f} GB\")\n\nllm = LLM_Engine(model_name=model_name)\nprint(\"\\nLLM ë¡œë“œ ì™„ë£Œ!\")\n\n# GPU ë©”ëª¨ë¦¬ ë¡œë“œ í›„ ì¸¡ì •\nif torch.cuda.is_available():\n    torch.cuda.synchronize()\n    mem_after = torch.cuda.memory_allocated()\n    print(f\"[ëª¨ë¸ ë¡œë“œ í›„] GPU ë©”ëª¨ë¦¬: {mem_after / 1024**3:.3f} GB\")\n    print(f\"[ëª¨ë¸ í¬ê¸°] {(mem_after - mem_before) / 1024**3:.3f} GB\")"
  },
  {
   "cell_type": "markdown",
   "source": "### 4-1. ëª¨ë¸ ë””ë°”ì´ìŠ¤ ìœ„ì¹˜ ë° ìƒì„¸ ì •ë³´",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# LLM_Engineì´ ì‚¬ìš©í•˜ëŠ” ë””ë°”ì´ìŠ¤ í™•ì¸\nprint(\"=\" * 60)\nprint(\"ëª¨ë¸ ë””ë°”ì´ìŠ¤ ë° ìƒì„¸ ì •ë³´\")\nprint(\"=\" * 60)\n\nprint(f\"\\n[1] LLM_Engine.device: {llm.device}\")\n\n# ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì–´ë””ì— ìˆëŠ”ì§€ í™•ì¸\nprint(f\"\\n[2] ëª¨ë¸ íŒŒë¼ë¯¸í„° ìœ„ì¹˜:\")\ntry:\n    # ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ì˜ ë””ë°”ì´ìŠ¤ í™•ì¸\n    first_param = next(llm.model.parameters())\n    print(f\"    ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„° device: {first_param.device}\")\n    print(f\"    dtype: {first_param.dtype}\")\n    \n    # ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ê°™ì€ ë””ë°”ì´ìŠ¤ì— ìˆëŠ”ì§€ í™•ì¸\n    devices = set()\n    for name, param in llm.model.named_parameters():\n        devices.add(str(param.device))\n    print(f\"    ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤ë“¤: {devices}\")\n    \n    if len(devices) == 1 and \"cuda\" in list(devices)[0]:\n        print(\"\\n    âœ“ ëª¨ë¸ì´ GPUì— ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n    elif len(devices) == 1 and \"cpu\" in list(devices)[0]:\n        print(\"\\n    âœ— ëª¨ë¸ì´ CPUì— ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤!\")\n        print(\"      â†’ GPUê°€ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì›ì¸ì…ë‹ˆë‹¤.\")\n    else:\n        print(f\"\\n    ëª¨ë¸ì´ ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤ì— ë¶„ì‚°ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {devices}\")\nexcept Exception as e:\n    print(f\"    íŒŒë¼ë¯¸í„° í™•ì¸ ì‹¤íŒ¨: {e}\")\n\n# ëª¨ë¸ config ì •ë³´\nprint(f\"\\n[3] ëª¨ë¸ ì •ë³´:\")\nprint(f\"    model_type: {getattr(llm.model.config, 'model_type', 'unknown')}\")\nprint(f\"    hidden_size: {getattr(llm.model.config, 'hidden_size', 'unknown')}\")\nprint(f\"    num_layers: {getattr(llm.model.config, 'num_hidden_layers', 'unknown')}\")\n\n# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\ntotal_params = sum(p.numel() for p in llm.model.parameters())\nprint(f\"    ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,} ({total_params / 1e9:.2f}B)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í”„ë¡¬í”„íŠ¸ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.llm import build_prompt\n",
    "\n",
    "user_input = \"í”¼í•´ì ê°€ì¡±ì—ê²Œ ê·¸ë‚  ìˆì—ˆë˜ ì¼ì„ ë¬¼ì–´ë³¸ë‹¤\"\n",
    "\n",
    "prompt = build_prompt(\n",
    "    user_input=user_input,\n",
    "    world_state=world.to_dict(),\n",
    "    memory_summary=None,\n",
    "    npc_context=assets.export_for_prompt(),\n",
    ")\n",
    "\n",
    "print(f\"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ì\")\n",
    "print(\"=\" * 60)\n",
    "print(prompt)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. tool_turn_resolution í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "source": "import time\nimport threading\n\ndef monitor_gpu_during_inference():\n    \"\"\"Inference ì¤‘ GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ (ë³„ë„ ìŠ¤ë ˆë“œ)\"\"\"\n    max_util = 0\n    max_mem = 0\n    samples = []\n    \n    def sample():\n        nonlocal max_util, max_mem\n        while getattr(monitor_gpu_during_inference, 'running', True):\n            try:\n                result = subprocess.run(\n                    [\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.used\", \"--format=csv,noheader,nounits\"],\n                    capture_output=True, text=True, timeout=1\n                )\n                if result.returncode == 0:\n                    parts = [p.strip() for p in result.stdout.strip().split(\",\")]\n                    if len(parts) >= 2:\n                        util = int(parts[0])\n                        mem = int(parts[1])\n                        max_util = max(max_util, util)\n                        max_mem = max(max_mem, mem)\n                        samples.append((util, mem))\n            except:\n                pass\n            time.sleep(0.1)  # 100ms ê°„ê²© ìƒ˜í”Œë§\n    \n    return sample, lambda: (max_util, max_mem, samples)\n\nprint(\"=\" * 60)\nprint(\"Inference ì¤‘ GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§\")\nprint(\"=\" * 60)\n\n# GPU ëª¨ë‹ˆí„°ë§ ì‹œì‘\nsampler, get_stats = monitor_gpu_during_inference()\nmonitor_gpu_during_inference.running = True\nmonitor_thread = threading.Thread(target=sampler, daemon=True)\nmonitor_thread.start()\n\n# inference ì „ ìƒíƒœ\nif torch.cuda.is_available():\n    torch.cuda.synchronize()\n    mem_before_inference = torch.cuda.memory_allocated()\n    torch.cuda.reset_peak_memory_stats()\n\nprint(f\"\\n[Inference ì „] GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.3f} GB\")\n\n# inference ì‹¤í–‰\nstart_time = time.perf_counter()\ntest_prompt = \"ì•ˆë…•í•˜ì„¸ìš”\"\nprint(f\"í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: '{test_prompt}'\")\nprint(\"Inference ì‹¤í–‰ ì¤‘...\")\n\ntest_output = llm.generate(test_prompt)\n\nend_time = time.perf_counter()\ninference_time = end_time - start_time\n\n# ëª¨ë‹ˆí„°ë§ ì¤‘ì§€\nmonitor_gpu_during_inference.running = False\ntime.sleep(0.2)  # ë§ˆì§€ë§‰ ìƒ˜í”Œ ìˆ˜ì§‘ ëŒ€ê¸°\n\n# ê²°ê³¼ ìˆ˜ì§‘\nmax_util, max_mem, samples = get_stats()\n\n# inference í›„ ìƒíƒœ\nif torch.cuda.is_available():\n    torch.cuda.synchronize()\n    mem_after_inference = torch.cuda.memory_allocated()\n    peak_mem = torch.cuda.max_memory_allocated()\n\nprint(f\"\\n[ê²°ê³¼]\")\nprint(f\"    Inference ì‹œê°„: {inference_time:.2f}ì´ˆ\")\nprint(f\"    ì¶œë ¥ ê¸¸ì´: {len(test_output)}ì\")\n\nif torch.cuda.is_available():\n    print(f\"\\n[GPU ë©”ëª¨ë¦¬]\")\n    print(f\"    inference ì „: {mem_before_inference / 1024**3:.3f} GB\")\n    print(f\"    inference í›„: {mem_after_inference / 1024**3:.3f} GB\")\n    print(f\"    peak: {peak_mem / 1024**3:.3f} GB\")\n\nprint(f\"\\n[nvidia-smi ëª¨ë‹ˆí„°ë§ ê²°ê³¼]\")\nprint(f\"    ìƒ˜í”Œ ìˆ˜: {len(samples)}\")\nif samples:\n    avg_util = sum(s[0] for s in samples) / len(samples)\n    avg_mem = sum(s[1] for s in samples) / len(samples)\n    print(f\"    í‰ê·  GPU ì‚¬ìš©ë¥ : {avg_util:.1f}%\")\n    print(f\"    ìµœëŒ€ GPU ì‚¬ìš©ë¥ : {max_util}%\")\n    print(f\"    í‰ê·  GPU ë©”ëª¨ë¦¬: {avg_mem:.0f} MB\")\n    print(f\"    ìµœëŒ€ GPU ë©”ëª¨ë¦¬: {max_mem} MB\")\n    \n    if max_util < 5:\n        print(\"\\n    âš  GPU ì‚¬ìš©ë¥ ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ (< 5%)!\")\n        print(\"      â†’ ë¡œì»¬ ëª¨ë¸ì´ ì‹¤ì œë¡œ GPUì—ì„œ ì‹¤í–‰ë˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.\")\n    elif max_util < 30:\n        print(\"\\n    âš  GPU ì‚¬ìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ (< 30%).\")\n        print(\"      â†’ ëª¨ë¸ì´ GPUì— ìˆì§€ë§Œ, ë³‘ëª©ì´ ë‹¤ë¥¸ ê³³ì— ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n    else:\n        print(\"\\n    âœ“ GPUê°€ í™œë°œí•˜ê²Œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\")\nelse:\n    print(\"    ìƒ˜í”Œì„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n\nprint(f\"\\n[ìƒì„±ëœ í…ìŠ¤íŠ¸ (ì²˜ìŒ 200ì)]\")\nprint(\"-\" * 40)\nprint(test_output[:200] if len(test_output) > 200 else test_output)\nprint(\"-\" * 40)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.tools import tool_turn_resolution\n",
    "\n",
    "user_input = \"í”¼í•´ì ê°€ì¡±ì—ê²Œ ê·¸ë‚  ìˆì—ˆë˜ ì¼ì„ ë¬¼ì–´ë³¸ë‹¤\"\n",
    "\n",
    "print(f\"ì…ë ¥: {user_input}\")\n",
    "print(\"LLM ìƒì„± ì¤‘...\")\n",
    "\n",
    "result = tool_turn_resolution(user_input, world, assets, llm)\n",
    "\n",
    "print(\"\\nê²°ê³¼:\")\n",
    "print(f\"  event_description: {result.event_description}\")\n",
    "print(f\"  state_delta: {result.state_delta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    \"í”¼í•´ì ê°€ì¡±ì—ê²Œ ê·¸ë‚  ìˆì—ˆë˜ ì¼ì„ ë¬¼ì–´ë³¸ë‹¤\",\n",
    "    \"íŒ¨í„´ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œë‹¤\",\n",
    "    \"í˜„ì¥ì„ ì¡°ì‚¬í•œë‹¤\",\n",
    "]\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ({len(test_cases)}ê°œ)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(test_cases, 1):\n",
    "    print(f\"\\n[{i}] ì…ë ¥: \\\"{text}\\\"\")\n",
    "    result = tool_turn_resolution(text, world, assets, llm)\n",
    "    print(f\"    ì‚¬ê±´: {result.event_description}\")\n",
    "    print(f\"    ë¸íƒ€: {result.state_delta}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Raw LLM ì¶œë ¥ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.llm import parse_response\n",
    "\n",
    "user_input = \"ëª©ê²©ìì—ê²Œ ë²”ì¸ì˜ ì¸ìƒì°©ì˜ë¥¼ ë¬¼ì–´ë³¸ë‹¤\"\n",
    "\n",
    "prompt = build_prompt(\n",
    "    user_input=user_input,\n",
    "    world_state=world.to_dict(),\n",
    "    memory_summary=None,\n",
    "    npc_context=assets.export_for_prompt(),\n",
    ")\n",
    "\n",
    "print(f\"ì…ë ¥: {user_input}\")\n",
    "print(\"\\nLLM ìƒì„± ì¤‘...\")\n",
    "\n",
    "raw_output = llm.generate(prompt)\n",
    "\n",
    "print(\"\\n[Raw LLM Output]\")\n",
    "print(\"-\" * 40)\n",
    "print(raw_output)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n[Parsed Response]\")\n",
    "parsed = parse_response(raw_output)\n",
    "print(f\"  state_delta: {parsed.state_delta}\")\n",
    "print(f\"  event_description: {parsed.event_description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 9. ë¡œì»¬ ëª¨ë¸ vs API í™•ì¸ ìš”ì•½",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"ì§„ë‹¨ ìš”ì•½\")\nprint(\"=\" * 60)\n\nissues = []\ninfo = []\n\n# 1. CUDA ê°€ìš©ì„± í™•ì¸\nif not torch.cuda.is_available():\n    issues.append(\"CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ - CPU ëª¨ë“œë¡œ ë™ì‘ ì¤‘\")\nelse:\n    info.append(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}\")\n\n# 2. ëª¨ë¸ ë””ë°”ì´ìŠ¤ í™•ì¸\ntry:\n    first_param = next(llm.model.parameters())\n    if \"cpu\" in str(first_param.device):\n        issues.append(\"ëª¨ë¸ì´ CPUì— ë¡œë“œë˜ì–´ ìˆìŒ\")\n    else:\n        info.append(f\"ëª¨ë¸ ë””ë°”ì´ìŠ¤: {first_param.device}\")\nexcept:\n    issues.append(\"ëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸ ì‹¤íŒ¨\")\n\n# 3. GPU ë©”ëª¨ë¦¬ í™•ì¸\nif torch.cuda.is_available():\n    gpu_mem = torch.cuda.memory_allocated() / 1024**3\n    if gpu_mem < 0.1:\n        issues.append(f\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§¤ìš° ë‚®ìŒ ({gpu_mem:.3f} GB)\")\n    else:\n        info.append(f\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: {gpu_mem:.2f} GB\")\n\n# 4. LangChain ì—”ì§„ ì‚¬ìš© ì—¬ë¶€ í™•ì¸\nprint(\"\\n[LLM ì—”ì§„ íƒ€ì… í™•ì¸]\")\nprint(f\"    ì‚¬ìš© ì¤‘ì¸ ì—”ì§„: {type(llm).__name__}\")\nif type(llm).__name__ == \"LLM_Engine\":\n    info.append(\"LLM_Engine (ë¡œì»¬ transformers) ì‚¬ìš© ì¤‘\")\nelif \"LangChain\" in type(llm).__name__:\n    issues.append(\"LangChainEngine (API) ì‚¬ìš© ì¤‘ - ë¡œì»¬ GPU ë¯¸ì‚¬ìš©!\")\n\n# ê²°ê³¼ ì¶œë ¥\nprint(\"\\n\" + \"-\" * 60)\nif issues:\n    print(\"\\nâš  ë°œê²¬ëœ ë¬¸ì œì :\")\n    for issue in issues:\n        print(f\"    - {issue}\")\nelse:\n    print(\"\\nâœ“ ë¬¸ì œì  ì—†ìŒ\")\n\nprint(\"\\nğŸ“‹ ì‹œìŠ¤í…œ ì •ë³´:\")\nfor item in info:\n    print(f\"    - {item}\")\n\n# ìµœì¢… íŒì •\nprint(\"\\n\" + \"-\" * 60)\nprint(\"\\n[ìµœì¢… íŒì •]\")\n\nif not torch.cuda.is_available():\n    print(\"    âŒ ë¡œì»¬ GPU ë¯¸ì‚¬ìš©: CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n    print(\"       â†’ PyTorch CUDA ë²„ì „ ì¬ì„¤ì¹˜ í•„ìš”\")\nelif \"cpu\" in str(next(llm.model.parameters()).device):\n    print(\"    âŒ ë¡œì»¬ GPU ë¯¸ì‚¬ìš©: ëª¨ë¸ì´ CPUì— ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n    print(\"       â†’ LLM_Engineì˜ device ì„¤ì • í™•ì¸ í•„ìš”\")\nelif torch.cuda.memory_allocated() < 1024**3:  # 1GB ë¯¸ë§Œ\n    print(\"    âš  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë‚®ìŠµë‹ˆë‹¤.\")\n    print(\"       â†’ ëª¨ë¸ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜, ë‹¤ë¥¸ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\nelse:\n    print(\"    âœ“ ë¡œì»¬ GPU ì‚¬ìš© ì¤‘: ëª¨ë¸ì´ GPUì—ì„œ ì‹¤í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤.\")\n    print(\"       â†’ GPU ì‚¬ìš©ë¥ ì´ ë‚®ë‹¤ë©´, inference ì‹œê°„ì´ ì§§ê±°ë‚˜ I/O ë³‘ëª©ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}